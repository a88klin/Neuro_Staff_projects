{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oB9T_33W8rG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Переводчики на контурных моделях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMqRkjTim-FS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## mBART-50 (1024) Transformers (Hugging Face).\n",
    "https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pgb1cpQiyZ-q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Установка зависимостей\n",
    "\n",
    "!pip install transformers==4.35.0\n",
    "!pip install sentencepiece==0.1.99\n",
    "!pip install sacremoses==0.1.1\n",
    "!pip install accelerate==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261640,
     "status": "ok",
     "timestamp": 1709394658269,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "NSRca1JDQWMY",
    "outputId": "e1fcce51-8b72-4ca0-a359-46554de8ac55",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n",
      "Cloning into 'mbart-large-50-many-to-many-mmt'...\n",
      "remote: Enumerating objects: 43, done.\u001B[K\n",
      "remote: Total 43 (delta 0), reused 0 (delta 0), pack-reused 43\u001B[K\n",
      "Unpacking objects: 100% (43/43), 7.57 KiB | 516.00 KiB/s, done.\n",
      "Filtering content: 100% (6/6), 11.38 GiB | 44.87 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6rhF1Rw6iBt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Использование конвейера (pipeline)\n",
    "from transformers import pipeline\n",
    "\n",
    "# \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "pipe = pipeline(\"translation\",\n",
    "                model=\"/content/mbart-large-50-many-to-many-mmt\",\n",
    "                src_lang=\"ru_RU\",\n",
    "                tgt_lang=\"en_XX\",\n",
    "                max_length=1024,\n",
    "                device='cuda')\n",
    "text = '...'\n",
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3654,
     "status": "ok",
     "timestamp": 1709394781336,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "JiK4WDsVYB_N",
    "outputId": "acc2be04-d287-4ff4-8a6d-2caf4df3f8a6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов во входной последовательности: 18\n",
      "[250004, 62, 19336, 7175, 111, 66733, 6, 130687, 25188, 1314, 99, 70, 43975, 100, 235398, 159415, 214, 2]\n",
      "['en_XX', 'A', 'small', 'team', 'of', 'artificial', '', 'intelligence', 'research', 'ers', 'at', 'the', 'Institute', 'for', 'Intelligent', 'Comput', 'ing', '</s>']\n"
     ]
    }
   ],
   "source": [
    "#@title Токены\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "\n",
    "def tokens_mBART50(text, lang=\"en_XX\"):\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"/content/mbart-large-50-many-to-many-mmt\")\n",
    "    tokenizer.src_lang = lang\n",
    "    token_list = tokenizer.encode(text)\n",
    "    # К-во токенов, список токенов, список частей текста\n",
    "    return len(token_list), token_list, [tokenizer.decode(n) for n in token_list]\n",
    "\n",
    "\n",
    "n, ln, ls = tokens_mBART50('A small team of artificial intelligence researchers at the Institute for Intelligent Computing')\n",
    "print(\"Количество токенов во входной последовательности:\", n)\n",
    "print(ln)\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3575,
     "status": "ok",
     "timestamp": 1709394807770,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "6BEuJj3QpkoT",
    "outputId": "4c75f70b-2361-4725-95e9-661d43738f24",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов во входном тексте: 307\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"A small team of artificial intelligence researchers at the Institute for Intelligent Computing, Alibaba Group,\n",
    "demonstrates, via videos they created, a new AI app that can accept a single photograph of a person's face and a soundtrack\n",
    "of someone speaking or singing and use them to create an animated version of the person speaking or singing the voice track.\n",
    "The group has published a paper describing their work on the arXiv preprint server.\n",
    "\n",
    "Prior researchers have demonstrated AI applications that can process a photograph of a face and use it to create a\n",
    "semi-animated version. In this new effort, the team at Alibaba has taken this a step further by adding sound. And perhaps,\n",
    "just as importantly, they have done so without the use of 3D models or even facial landmarks. Instead, the team has used\n",
    "diffusion modeling based on training an AI on large datasets of audio or video files. In this instance, the team used\n",
    "approximately 250 hours of such data to create their app, which they call Emote Portrait Alive (EMO).\n",
    "\n",
    "By directly converting the audio waveform into video frames, the researchers created an application that captures subtle\n",
    "human facial gestures, quirks of speech and other characteristics that identify an animated image of a face as human-like.\n",
    "The videos faithfully recreate the likely mouth shapes used to form words and sentences, along with expressions typically\n",
    "associated with them.\"\"\"\n",
    "\n",
    "\n",
    "print(\"Количество токенов во входном тексте:\", tokens_mBART50(text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVPS-T98QMPQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Ручная загрузка модели\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(\n",
    "                   \"/content/mbart-large-50-many-to-many-mmt\").to(device)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "                   \"/content/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "\n",
    "def tranlate_mBART50(text, tokenizer, device, model, input_lang_code=\"en_XX\", output_lang_code=\"ru_RU\"):\n",
    "    tokenizer.src_lang = input_lang_code\n",
    "    encoded_ru = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = model.generate(\n",
    "                **encoded_ru,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[output_lang_code],\n",
    "                max_length=1024)\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25003,
     "status": "ok",
     "timestamp": 1709394918480,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "99YFb5snugw-",
    "outputId": "6a0773dd-876e-452b-8d01-d26129cf654a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['Небольшая команда исследователей искусственного интеллекта в Институте интеллектуального вычисления, Alibaba Group, демонстрирует, через видео, которые они создали, новую программу искусственного интеллекта, которая может принять одну фотографию лица человека и саундтрек кого-то говорить или петь и использовать их для создания анимированной версии человека говорить или петь голос трек. Группа опубликовала статью, описывающую свою работу на arXiv предварительного печати сервера. Предыдущие исследователи продемонстрировали приложения искусственного интеллекта, которые могут обрабатывать фотографию лица и использовать ее для создания полуанимированной версии. В этом новом усилии, команда в Alibaba взяла этот шаг дальше путем добавления звука. И, возможно, столь же важно, они сделали это без использования 3D моделей или даже линейных знаков. Вместо этого, команда использовала диффузионное моделирование, основанное на обучении искусственного интеллекта на больших наборах данных аудио или видео файлов. В этом случае, команда использовала примерно 250 часов таких данных для создания своей программы, которую они называют Emote Portrait Alive (EMO).']\n",
      "\n",
      " ['Un pequeño equipo de investigadores de inteligencia artificial en el Instituto de Computación Intelligente, Alibaba Group, muestra, a través de los vídeos que han creado, una nueva aplicación de inteligencia artificial que puede aceptar una foto única de la cara de una persona y una banda sonora de alguien hablando o cantando y utilizarlas para crear una versión animada de la persona hablando o cantando la pista de voz. El grupo ha publicado un documento describiendo su trabajo en el servidor de pre impresión arXiv. Anteriormente, los investigadores han demostrado aplicaciones de inteligencia artificial que pueden procesar una foto de una cara y usarla para crear una versión semi-animada. En este nuevo esfuerzo, el equipo de Alibaba ha dado este paso más lejos al añadir sonido.']\n",
      "\n",
      " [\"Un petit groupe de chercheurs en intelligence artificielle de l'Institut de calculs intelligents, Alibaba Group, démontre, par le biais de vidéos qu'ils ont créées, une nouvelle application d'AI qui peut accepter une seule photographie du visage d'une personne et une bande sonore de quelqu'un qui parle ou chante et les utiliser pour créer une version animée de la personne qui parle ou chante la piste vocale. Le groupe a publié un document décrivant son travail sur le serveur de préimpression arXiv. Les chercheurs antérieurs ont démontré des applications d'AI qui peuvent traiter une photographie d'un visage et l'utiliser pour créer une version semi-animée. Dans ce nouvel effort, l'équipe d'Alibaba a fait un pas plus loin en ajoutant du son. Et peut-être, tout aussi important, ils l'ont fait sans l'utilisation de modèles 3D ou même de repères faciales. Au lieu de cela, l'équipe a utilisé la modélisation de diffusion fondée sur la formation d'une AI sur de grands ensembles de données d'audio ou de fichiers vidéo. Dans ce cas, l'équipe a utilisé environ 250 heures de ces données pour créer leur application, qu'elle appelle Emote Portrait Alive (EMO). En convertissant directement la forme d'onde audio en cadres vidéo, les chercheurs ont créé une application qui capte des gestes faciales humains subtils, des gestes de parole et d'autres caractéristiques qui identifient une image animée d'un visage comme humain. Les vidéos récréent fidèlement les formes orales probables utilisées pour former des mots et des phrases, ainsi que des expressions habituellement associées à eux.\"]\n",
      "CPU times: user 23.8 s, sys: 87.3 ms, total: 23.9 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title mBART-50 - Russian, Spanish, French\n",
    "lang_codes = ['ru_RU', 'es_XX', 'fr_XX']\n",
    "for lang_code in lang_codes:\n",
    "    print('\\n', tranlate_mBART50(text, tokenizer, device, model, output_lang_code=lang_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RKVBmW74wYx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Входная последовательность на английском 307 токенов, но модель mBART-50 почему-то не перевела до конца на русский и испанский. На французский полностью до конца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrB1-0KuEeOT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saiga\n",
    "\n",
    "### https://huggingface.co/IlyaGusev\n",
    "### https://huggingface.co/IlyaGusev/saiga2_7b_gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5glAwL0pbnAk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.35.0\n",
    "!pip install sentencepiece==0.1.99\n",
    "!pip install sacremoses==0.1.1\n",
    "!pip install accelerate==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iD1atoGziBIE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43554,
     "status": "ok",
     "timestamp": 1709400517505,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "vstyJhbVEeeN",
    "outputId": "fff74ec1-68f2-4278-a76f-bf5ecce4d4b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-02 17:27:53--  https://huggingface.co/IlyaGusev/saiga2_7b_gguf/resolve/main/model-q2_K.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.70, 99.84.108.87, 99.84.108.129, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/5c/51/5c517a9e7f5c6a5b0428792a82e7a380cfeaabb4d93e66cbb82f023ed030f30d/ecdb8ac229f6cdc3f21d5a3e95dcd26f31c355b727299e96b44ab7a7dbc34fb4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-q2_K.gguf%3B+filename%3D%22model-q2_K.gguf%22%3B&Expires=1709659673&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTY1OTY3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81Yy81MS81YzUxN2E5ZTdmNWM2YTViMDQyODc5MmE4MmU3YTM4MGNmZWFhYmI0ZDkzZTY2Y2JiODJmMDIzZWQwMzBmMzBkL2VjZGI4YWMyMjlmNmNkYzNmMjFkNWEzZTk1ZGNkMjZmMzFjMzU1YjcyNzI5OWU5NmI0NGFiN2E3ZGJjMzRmYjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=CXmRbhgYHXD0S4beXIJNKga4JUQEyEW%7EpGPHFjMochJo1CYrKdOJjP8B%7Elk-%7EQ6n9tzSIFL0-OAauEIouzJ1%7ERCYIdRpkpYcVP5fC7PzTsdvXZD1mLNigzuN5ORSN4rmKTUzWE8dIkBB0b6m0ujYo%7EH2TOa4RfhjJvHswTrkTU3USZaYyDunsUL3nbwY795TCH2mIcQJJ7HDwtSIJlnAzwLV737Ho%7Elro6nfqUhhYrCiXc7RkJCFwYbNk0WVb8MbUNKZ9LnOMw9ReOytWLyelHX2Le2F1nH77MIWnWGYTpwurPRIgR7YZw0GRA530Eti1RvfIGlxRY6xHYj7ZZn95g__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-03-02 17:27:53--  https://cdn-lfs.huggingface.co/repos/5c/51/5c517a9e7f5c6a5b0428792a82e7a380cfeaabb4d93e66cbb82f023ed030f30d/ecdb8ac229f6cdc3f21d5a3e95dcd26f31c355b727299e96b44ab7a7dbc34fb4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-q2_K.gguf%3B+filename%3D%22model-q2_K.gguf%22%3B&Expires=1709659673&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTY1OTY3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81Yy81MS81YzUxN2E5ZTdmNWM2YTViMDQyODc5MmE4MmU3YTM4MGNmZWFhYmI0ZDkzZTY2Y2JiODJmMDIzZWQwMzBmMzBkL2VjZGI4YWMyMjlmNmNkYzNmMjFkNWEzZTk1ZGNkMjZmMzFjMzU1YjcyNzI5OWU5NmI0NGFiN2E3ZGJjMzRmYjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=CXmRbhgYHXD0S4beXIJNKga4JUQEyEW%7EpGPHFjMochJo1CYrKdOJjP8B%7Elk-%7EQ6n9tzSIFL0-OAauEIouzJ1%7ERCYIdRpkpYcVP5fC7PzTsdvXZD1mLNigzuN5ORSN4rmKTUzWE8dIkBB0b6m0ujYo%7EH2TOa4RfhjJvHswTrkTU3USZaYyDunsUL3nbwY795TCH2mIcQJJ7HDwtSIJlnAzwLV737Ho%7Elro6nfqUhhYrCiXc7RkJCFwYbNk0WVb8MbUNKZ9LnOMw9ReOytWLyelHX2Le2F1nH77MIWnWGYTpwurPRIgR7YZw0GRA530Eti1RvfIGlxRY6xHYj7ZZn95g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.111, 108.138.64.121, 108.138.64.49, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.111|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2825940544 (2.6G) [binary/octet-stream]\n",
      "Saving to: ‘model-q2_K.gguf’\n",
      "\n",
      "model-q2_K.gguf     100%[===================>]   2.63G  54.3MB/s    in 43s     \n",
      "\n",
      "2024-03-02 17:28:36 (63.0 MB/s) - ‘model-q2_K.gguf’ saved [2825940544/2825940544]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://huggingface.co/IlyaGusev/saiga2_7b_gguf/resolve/main/model-q2_K.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik1jRCBrTn7M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Saiga. Токены\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga_mistral_7b_lora\")\n",
    "\n",
    "\n",
    "def tokens_saiga(text, tokenizer, lang):\n",
    "    tokenizer.src_lang = lang\n",
    "    token_list = tokenizer.encode(text)\n",
    "    # К-во токенов, список токенов, список частей текста\n",
    "    return len(token_list), token_list, [tokenizer.decode(n) for n in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1709405244433,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "XnluF9zUbElT",
    "outputId": "2ca17780-cbb9-47c5-8ecd-800b87d9fc1e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 32\n",
      "[12112, 2290, 1049, 25356, 11015, 1225, 17282, 23050, 16871, 839, 11249, 28788, 7265, 3111, 8257, 12516, 1125, 16702, 649, 18170, 17356, 1078, 8257, 12516, 1125, 19456, 8067, 4254, 4627, 2348, 5333, 3962]\n",
      "['Не', 'бо', 'ль', 'шая', 'коман', 'да', 'ис', 'следова', 'телей', 'и', 'ску', 'с', 'ствен', 'ного', 'ин', 'тел', 'ле', 'кта', 'в', 'Ин', 'ститу', 'те', 'ин', 'тел', 'ле', 'кту', 'аль', 'ных', 'вы', 'чи', 'сле', 'ний']\n"
     ]
    }
   ],
   "source": [
    "string = 'Небольшая команда исследователей искусственного интеллекта в Институте интеллектуальных вычислений'\n",
    "n, ln, ls = tokens_saiga(string, tokenizer, lang=\"ru_RU\")\n",
    "print(\"Количество токенов:\", n)\n",
    "print(ln)\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1709405248941,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "jHO3XJJiXgdF",
    "outputId": "1ec4b619-59b2-4d58-d500-7beaeea2e6c2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 16\n",
      "[330, 1741, 1918, 302, 18278, 10895, 15334, 438, 272, 8624, 354, 4666, 6095, 308, 13651, 288]\n",
      "['A', 'small', 'team', 'of', 'artificial', 'intelligence', 'researchers', 'at', 'the', 'Institute', 'for', 'Int', 'ellig', 'ent', 'Comput', 'ing']\n"
     ]
    }
   ],
   "source": [
    "string = 'A small team of artificial intelligence researchers at the Institute for Intelligent Computing'\n",
    "n, ln, ls = tokens_saiga(string, tokenizer, lang=\"en_EN\")\n",
    "print(\"Количество токенов:\", n)\n",
    "print(ln)\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDMGTpMgEs9R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title  Перевод Saiga\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "SYSTEM_TOKEN = 1788\n",
    "USER_TOKEN = 1404\n",
    "BOT_TOKEN = 9225\n",
    "LINEBREAK_TOKEN = 13\n",
    "\n",
    "ROLE_TOKENS = {\n",
    "    \"user\": USER_TOKEN,\n",
    "    \"bot\": BOT_TOKEN,\n",
    "    \"system\": SYSTEM_TOKEN\n",
    "}\n",
    "\n",
    "def get_message_tokens(model, role, content):\n",
    "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
    "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
    "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
    "    message_tokens.append(model.token_eos())\n",
    "    return message_tokens\n",
    "\n",
    "def get_system_tokens(model):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT\n",
    "    }\n",
    "    return get_message_tokens(model, **system_message)\n",
    "\n",
    "def interact(\n",
    "    model_path,\n",
    "    prompt,\n",
    "    n_ctx=4096,  # Context window\n",
    "    max_tokens=2000,  # Max tokens to generate\n",
    "    n_gpu_layers = 2000,  # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_batch = 512,\n",
    "    top_k=30,\n",
    "    top_p=0.9,\n",
    "    temperature=0.2,\n",
    "    repeat_penalty=1.1\n",
    "):\n",
    "    model = Llama(\n",
    "        model_path=model_path,\n",
    "        n_ctx=n_ctx,\n",
    "        n_parts=1,\n",
    "        max_tokens=max_tokens,\n",
    "        n_gpu_layers = n_gpu_layers,\n",
    "        n_batch = n_batch,\n",
    "    )\n",
    "\n",
    "    system_tokens = get_system_tokens(model)\n",
    "    tokens = system_tokens\n",
    "    model.eval(tokens)\n",
    "\n",
    "    user_message = prompt\n",
    "    message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
    "    role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
    "    tokens += message_tokens + role_tokens\n",
    "    print('Количество токенов на вход: ', len(tokens))\n",
    "    generator = model.generate(\n",
    "        tokens,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temp=temperature,\n",
    "        repeat_penalty=repeat_penalty\n",
    "    )\n",
    "    print('\\n\\n')\n",
    "    for token in generator:\n",
    "        token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
    "        tokens.append(token)\n",
    "        if token == model.token_eos():\n",
    "            break\n",
    "        print(token_str, end=\"\", flush=True)\n",
    "    print('\\nКоличество всего токенов: ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26441,
     "status": "ok",
     "timestamp": 1709399166510,
     "user": {
      "displayName": "Alex Klin",
      "userId": "11249935885368047783"
     },
     "user_tz": -180
    },
    "id": "tfrzQ7qGkj3h",
    "outputId": "8bfc3e3f-9db4-4390-c38f-ee71dccc1f79",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /content/model-q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    41.02 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2653.31 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    17.04 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'general.file_type': '10', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'general.name': 'LLaMA v2', 'llama.block_count': '32', 'llama.context_length': '4096', 'general.architecture': 'llama'}\n",
      "Using fallback chat format: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов на вход:  408\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исследовательская группа маленького отдела искусственного интеллекта Института для компьютерной обработки, Алибаба Групп, демонстрирует, через видео, которые они создали, новую программу искусственного интеллекта, которая может принять одну фотографию лица и звуковой трек, чтобы использовать их для создания анимированной версии голоса или пения. Группа опубликовала описание своей работы на сервисе arXiv препрофил.\n",
      "Предыдущие исследователи демонстрировали приложения искусственного интеллекта, которые могут обрабатывать фотографию лица и использовать ее для создания полу-анимированной версии. В этом новом усилии, группа Алибаба взяла это еще дальше, добавив звуковой трек. И, возможно, еще важнее всего, они использовали без использования 3D моделей или даже лицевых мест. Вместо этого, группа использовала моделирование рассеивания, основанное на обучении АИ на больших базах данных звука или видео. В этом случае группа использовала приблизительно 250 часов таких данных для создания своего приложения, которое они называют Emote Portrait Alive (EMO).\n",
      "\n",
      "По прямому обращению звуковой волны в видео кадры, группа создала видео, которые верно воспроизводят малые движения губ, характерные для речи, и другие характеристики, которые идентифицируют анимированное изображение лица как человеческое. Видео верно воспроизводят малые движения губ, характерные для речи, и другие характеристики, которые идентифицируют анимированное изображение лица как человеческое. Группа использовала 250 часов таких данных для создания своего приложения, которое они называют Emote Portrait Alive (EMO).\n",
      "Количество всего токенов:  909\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"A small team of artificial intelligence researchers at the Institute for Intelligent Computing, Alibaba Group,\n",
    "demonstrates, via videos they created, a new AI app that can accept a single photograph of a person's face and a soundtrack\n",
    "of someone speaking or singing and use them to create an animated version of the person speaking or singing the voice track.\n",
    "The group has published a paper describing their work on the arXiv preprint server.\n",
    "\n",
    "Prior researchers have demonstrated AI applications that can process a photograph of a face and use it to create a\n",
    "semi-animated version. In this new effort, the team at Alibaba has taken this a step further by adding sound. And perhaps,\n",
    "just as importantly, they have done so without the use of 3D models or even facial landmarks. Instead, the team has used\n",
    "diffusion modeling based on training an AI on large datasets of audio or video files. In this instance, the team used\n",
    "approximately 250 hours of such data to create their app, which they call Emote Portrait Alive (EMO).\n",
    "\n",
    "By directly converting the audio waveform into video frames, the researchers created an application that captures subtle\n",
    "human facial gestures, quirks of speech and other characteristics that identify an animated image of a face as human-like.\n",
    "The videos faithfully recreate the likely mouth shapes used to form words and sentences, along with expressions typically\n",
    "associated with them.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Переведи текст на русский язык.\n",
    "Текст: {text}\n",
    "Текст на русском:\n",
    "\"\"\"\n",
    "\n",
    "interact(model_path=\"/content/model-q2_K.gguf\", prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}